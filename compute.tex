%%
%%  needs to update
%%

\section{Computational and management experience}

To do large scale numerical lattice QCD calculations that are of
interest to experimentalists, requires a computational infrastructure
to support calculations on supercomputers.  While I was paid on
'physicist programmer' and 'software manager' grants, I was crucial in
developing new strategies for the software development within the 
UKQCD collaboration.
Indeed the software manager position was explicitly created for me.
For two years I managed a 'physicist programmer' at Liverpool, who
was developing code for the QCDOC computer. 

While working in the UK, I was involved in the 
QCDgrid project (http://www.gridpp.ac.uk/qcdgrid/), 
an e-science project that
uses grid technology for lattice QCD.  I ensured that the software was
usable by the average physicist. Also I wrote requirements
documents for the developers of the software.

I have a lot of experience with performing lattice QCD calculations on
both large and small computers. 
I have run benchmark jobs on the 
BluegeneP at Darsebury Laboratory, production runs on
the Bluegene/P at Argonne National Lab and Julich supercomputer centre.
I have recently run on 
clusters at Fermilab, Cambridge and Julich.

I have
used EDG grid submission tools to run jobs on a compute grid
called Scotgrid.  While I was a postdoc in the US, I ran on
super-computers at NERSC, Cornell, Oakridge, SDSC, and Pittsburgh
supercomputer centre. I have used a number of open source codes such
as the MILC code and Chroma (largely written by people at the
Jefferson lab).

I am a member of the
HPQCD collaboration (a US-Canada-UK wide collaboration),
and BMW-c (Budapest-Marseille-Wuppertal).
In the past I have been a member of the UKQCD,
European Twisted Mass,
and MILC collaborations.
